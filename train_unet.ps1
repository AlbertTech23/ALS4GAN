# ============================================================
# UNet++ with MobileNetV2 Training - Advanced Lightweight Model
# ============================================================
#
# ARCHITECTURE:
# ‚Ä¢ UNet++ with MobileNetV2 encoder (BEST OF BOTH WORLDS)
# ‚Ä¢ Dense nested skip connections (superior to standard UNet)
# ‚Ä¢ Parameters: ~5-7M (7√ó lighter than DeepLabV3+)
# ‚Ä¢ Multi-Head ensemble (3 heads)
# ‚Ä¢ Expected: 0.65-0.70 mIoU (matching V3 with less compute)
#
# ADVANTAGES OVER STANDARD UNET:
# ‚Ä¢ Dense connections: Better feature fusion
# ‚Ä¢ More accurate: ~2-3% higher than standard UNet
# ‚Ä¢ Nested decoder: Gradual feature transformation
# ‚Ä¢ Still lightweight and fast
#
# IMPROVEMENTS FROM V3.0 (applied here):
# ‚úÖ Lowered confidence thresholds: 0.45-0.60
# ‚úÖ Patience: 40 evaluations (~20k iterations)
# ‚úÖ ST weight: 1.5 (stronger self-training)
# ‚úÖ Training: 75k iterations
# ‚úÖ Multi-scale: 256-320 (narrower range)
# ‚úÖ Learning rate: 0.0003
# ‚úÖ Warmup: 1500 iterations
# ‚úÖ Batch size: 12 (balanced for UNet++)
# ============================================================

Write-Host "============================================================" -ForegroundColor Cyan
Write-Host "UNet++ with MobileNetV2 Training - Salak Dataset" -ForegroundColor Cyan
Write-Host "============================================================" -ForegroundColor Cyan
Write-Host ""
Write-Host "üöÄ Advanced Lightweight Architecture" -ForegroundColor Green
Write-Host "  ‚Ä¢ UNet++ with dense nested skip connections" -ForegroundColor Yellow
Write-Host "  ‚Ä¢ Parameters: ~5-7M vs 40M (7√ó reduction)" -ForegroundColor Yellow
Write-Host "  ‚Ä¢ Better than standard UNet (~2-3% higher mIoU)" -ForegroundColor Yellow
Write-Host "  ‚Ä¢ Same V3.1 improvements applied" -ForegroundColor Yellow
Write-Host ""

# Configuration
$DATA_ROOT = "C:/_albert/s4GAN/patchify/temp_patches"
$CLASS_MAPPING = "C:/_albert/ALS4GAN/class_mapping.csv"
$CHECKPOINT_DIR = "C:/_albert/ALS4GAN/checkpoints_unetpp"
$WANDB_PROJECT = "als4gan-salak"
$WANDB_RUN_NAME = "unetpp_mobilenetv2_improved"

# V3.1 Improved Hyperparameters
$BATCH_SIZE = 12          # Balanced for UNet++ (slightly more params than UNet)
$NUM_STEPS = 75000        # Same as V3.1
$LEARNING_RATE = 0.0003   # Same as V3.1
$WARMUP_ITERS = 1500      # Same as V3.1
$ST_WEIGHT = 1.5          # Same as V3.1
$PATIENCE = 40            # Same as V3.1
$SCALE_MIN = 256          # Same as V3.1
$SCALE_MAX = 320          # Same as V3.1

Write-Host "üìã Configuration:" -ForegroundColor Cyan
Write-Host "  Data Root: $DATA_ROOT" -ForegroundColor White
Write-Host "  Checkpoint Dir: $CHECKPOINT_DIR" -ForegroundColor White
Write-Host "  Batch Size: $BATCH_SIZE (balanced for UNet++)" -ForegroundColor White
Write-Host "  Training Steps: $NUM_STEPS" -ForegroundColor White
Write-Host "  Learning Rate: $LEARNING_RATE" -ForegroundColor White
Write-Host "  Warmup Iterations: $WARMUP_ITERS" -ForegroundColor White
Write-Host "  ST Weight: $ST_WEIGHT" -ForegroundColor White
Write-Host "  Patience: $PATIENCE evaluations (~20k iterations)" -ForegroundColor White
Write-Host "  Multi-Scale: ${SCALE_MIN}-${SCALE_MAX}px" -ForegroundColor White
Write-Host ""

# Create checkpoint directory
if (-not (Test-Path $CHECKPOINT_DIR)) {
    Write-Host "üìÅ Creating checkpoint directory..." -ForegroundColor Yellow
    New-Item -ItemType Directory -Path $CHECKPOINT_DIR -Force | Out-Null
}

# Confirm before starting
Write-Host "‚ö†Ô∏è  Ready to start UNet++ training..." -ForegroundColor Yellow
Write-Host "   This will run for ~22-28 hours" -ForegroundColor Yellow
Write-Host "   Checkpoints saved to: $CHECKPOINT_DIR" -ForegroundColor Yellow
Write-Host ""
$response = Read-Host "Continue? (y/n)"
if ($response -ne "y") {
    Write-Host "‚ùå Training cancelled." -ForegroundColor Red
    exit
}

Write-Host ""
Write-Host "üöÄ Starting UNet++ with MobileNetV2 training..." -ForegroundColor Green
Write-Host ""

# Run training with V3.1 improvements
python tools/train_unet_salak.py `
  --data-root $DATA_ROOT `
  --class-mapping $CLASS_MAPPING `
  --num-classes 7 `
  --batch-size $BATCH_SIZE `
  --num-steps $NUM_STEPS `
  --learning-rate $LEARNING_RATE `
  --warmup-iters $WARMUP_ITERS `
  --st-loss-weight $ST_WEIGHT `
  --early-stop-patience $PATIENCE `
  --scale-min $SCALE_MIN `
  --scale-max $SCALE_MAX `
  --checkpoint-dir $CHECKPOINT_DIR `
  --wandb-project $WANDB_PROJECT `
  --wandb-run-name $WANDB_RUN_NAME `
  --use-multi-head `
  --use-combined-loss `
  --use-class-weights `
  --multi-scale `
  --use-ema `
  --random-mirror `
  --random-scale

$exitCode = $LASTEXITCODE
Write-Host ""
if ($exitCode -eq 0) {
    Write-Host "‚úÖ Training completed successfully!" -ForegroundColor Green
    Write-Host "üìä Check results in: $CHECKPOINT_DIR" -ForegroundColor Cyan
} else {
    Write-Host "‚ùå Training failed with exit code: $exitCode" -ForegroundColor Red
}

exit $exitCode
