"""
S4GAN-DeepLabV3+ Hybrid Training Script - Version 3
====================================================

MAJOR IMPROVEMENTS FROM V2:
1. âœ… DeepLabV3+ ResNet50 architecture (vs DeepLabV2 ResNet101)
2. âœ… Multi-Head diversity (inspired by DiverseNet)
3. âœ… Fixed pseudo-labeling (softmax confidence, not discriminator)
4. âœ… Combined loss (CE + Dice + Focal)
5. âœ… Class-wise confidence thresholds
6. âœ… Multi-scale training
7. âœ… Deep supervision
8. âœ… Class weighting for imbalance
9. âœ… All V2 stability features (EMA, gradient clipping, cosine LR)
10. âœ… NO adversarial discriminator (removed complexity)

EXPECTED PERFORMANCE:
- Target: 0.70-0.76 mIoU (matching DiverseNet)
- ST_cnt: 10,000-50,000 pixels/batch (vs <10 in V1/V2)
/- Stability: Â±5% fluctuation (vs Â±50% in V1)4b
- Train/Val gap: <0.08 consistently

Usage:
    python tools/train_s4gan_salak_v3.py \\
      --data-root "C:/_albert/s4GAN/patchify/temp_patches" \\
      --class-mapping "C:/_albert/ALS4GAN/class_mapping.csv" \\
      --num-classes 7 \\
      --batch-size 8 \\
      --num-steps 50000 \\
      --checkpoint-dir "C:/_albert/ALS4GAN/checkpoints_v3" \\
      --wandb-project "als4gan-salak"
"""

import argparse
import os
import numpy as np
import timeit
import sys
import copy
from collections import deque
from sklearn.utils.class_weight import compute_class_weight

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torch.backends.cudnn as cudnn
from torch.utils import data
from torch.autograd import Variable
from tqdm import tqdm

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from model.deeplabv3plus import DeepLabV3Plus_ResNet50, MultiHeadDeepLabV3Plus
from data.salak_dataset import SalakDataSet
from utils.loss import CrossEntropy2d, DiceLoss, FocalLoss, CombinedLoss
from utils.metric import scores

# Wandb for experiment tracking
import wandb

start = timeit.default_timer()

IMG_MEAN = np.array((104.00698793, 116.66876762, 122.67891434), dtype=np.float32)

# Default hyperparameters
INPUT_SIZE = '256,256'
IGNORE_LABEL = 255
BATCH_SIZE = 8
NUM_STEPS = 50000
SAVE_PRED_EVERY = 5000
SAVE_LATEST_EVERY = 100
EVAL_EVERY = 500
LEARNING_RATE = 2.5e-4
POWER = 0.9
WEIGHT_DECAY = 0.0005
MOMENTUM = 0.9
RANDOM_SEED = 5000

# V3 SPECIFIC
GRADIENT_CLIP_MAX_NORM = 10.0
EMA_DECAY = 0.9995  # Slightly higher for more stability
SELF_TRAINING_START_ITER = 500
EARLY_STOP_PATIENCE = 10


class ExponentialMovingAverage:
    """EMA for model stability"""
    def __init__(self, model, decay=0.9995):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        self.register()

    def register(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()

    def update(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.shadow
                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]
                self.shadow[name] = new_average.clone()

    def apply_shadow(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.shadow
                self.backup[name] = param.data
                param.data = self.shadow[name]

    def restore(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.backup
                param.data = self.backup[name]
        self.backup = {}


def get_arguments():
    """Parse CLI arguments"""
    parser = argparse.ArgumentParser(description="S4GAN-DeepLabV3+ V3 Training")
    
    # Dataset
    parser.add_argument("--data-root", type=str, required=True)
    parser.add_argument("--class-mapping", type=str, required=True)
    parser.add_argument("--num-classes", type=int, default=7)
    parser.add_argument("--labeled-files", type=str, default="labeled_files.txt", 
                        choices=["labeled_files.txt", "labeled_files_50.txt", "labeled_files_100.txt"],
                        help="Choose labeled files list: labeled_files.txt, labeled_files_50.txt, or labeled_files_100.txt")
    
    # Architecture
    parser.add_argument("--backbone", type=str, default="resnet50", choices=["resnet50", "resnet101"])
    parser.add_argument("--output-stride", type=int, default=16, choices=[8, 16])
    parser.add_argument("--num-heads", type=int, default=3, help="Number of classification heads for diversity")
    parser.add_argument("--use-multi-head", action="store_true", default=True, help="Use multi-head architecture")
    
    # Training
    parser.add_argument("--batch-size", type=int, default=BATCH_SIZE)
    parser.add_argument("--num-steps", type=int, default=NUM_STEPS)
    parser.add_argument("--learning-rate", type=float, default=LEARNING_RATE)
    parser.add_argument("--val-split", type=float, default=0.2)
    
    # Loss
    parser.add_argument("--use-combined-loss", action="store_true", default=True)
    parser.add_argument("--ce-weight", type=float, default=0.4)
    parser.add_argument("--dice-weight", type=float, default=0.4)
    parser.add_argument("--focal-weight", type=float, default=0.2)
    parser.add_argument("--use-class-weights", action="store_true", default=True)
    
    # Self-Training
    parser.add_argument("--confidence-threshold", type=float, default=0.65, 
                        help="Default confidence threshold (will use class-wise)")
    parser.add_argument("--use-classwise-threshold", action="store_true", default=True)
    parser.add_argument("--st-loss-weight", type=float, default=1.0)
    
    # Augmentation
    parser.add_argument("--multi-scale", action="store_true", default=True)
    parser.add_argument("--scale-min", type=int, default=256)
    parser.add_argument("--scale-max", type=int, default=384)
    parser.add_argument("--random-mirror", action="store_true", default=True)
    parser.add_argument("--random-scale", action="store_true", default=True)
    
    # V3 Specific
    parser.add_argument("--use-ema", action="store_true", default=True)
    parser.add_argument("--ema-decay", type=float, default=EMA_DECAY)
    parser.add_argument("--gradient-clip", type=float, default=GRADIENT_CLIP_MAX_NORM)
    parser.add_argument("--warmup-iters", type=int, default=1000)
    parser.add_argument("--early-stop-patience", type=int, default=EARLY_STOP_PATIENCE)
    
    # Checkpoints
    parser.add_argument("--checkpoint-dir", type=str, default="./checkpoints_v3")
    parser.add_argument("--restore-from", type=str, default=None)
    parser.add_argument("--save-pred-every", type=int, default=SAVE_PRED_EVERY)
    parser.add_argument("--save-latest-every", type=int, default=SAVE_LATEST_EVERY)
    parser.add_argument("--eval-every", type=int, default=EVAL_EVERY)
    
    # Wandb
    parser.add_argument("--wandb-project", type=str, default="als4gan-salak")
    parser.add_argument("--wandb-run-name", type=str, default=None)
    parser.add_argument("--no-wandb", action="store_true")
    
    # Other
    parser.add_argument("--input-size", type=str, default=INPUT_SIZE)
    parser.add_argument("--random-seed", type=int, default=RANDOM_SEED)
    parser.add_argument("--ignore-label", type=int, default=IGNORE_LABEL)
    
    return parser.parse_args()


def get_device(cuda=True):
    """Get CUDA device"""
    cuda = cuda and torch.cuda.is_available()
    device = torch.device("cuda" if cuda else "cpu")
    if cuda:
        print("Device:")
        for i in range(torch.cuda.device_count()):
            print(f"    {i}: {torch.cuda.get_device_name(i)}")
    else:
        print("Device: CPU")
    return device


def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5):
    """Cosine annealing with warmup"""
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
        return max(0.0, 0.5 * (1.0 + np.cos(np.pi * float(num_cycles) * 2.0 * progress)))
    
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)


def get_classwise_thresholds(num_classes):
    """
    Class-wise confidence thresholds for pseudo-labeling.
    Tuned based on class difficulty and frequency.
    
    V3.1 IMPROVEMENT: Lowered thresholds by 0.10 to use more pseudo-labels.
    Previous (V3.0) thresholds were too strict, causing early plateau at 0.62.
    """
    # Improved thresholds (lowered by 0.10 for better self-training)
    thresholds = {
        0: 0.0,   # Background (ignored anyway)
        1: 0.50,  # Badan Air (was 0.60) - rare class, be more permissive
        2: 0.55,  # Bangunan (was 0.65)
        3: 0.60,  # Jalan (was 0.70) - was too strict
        4: 0.55,  # Pohon Berinang (was 0.65)
        5: 0.45,  # Snake Fruit (was 0.55) - dominant class, learn more examples
        6: 0.50,  # Tanah Terbuka (was 0.60)
    }
    return thresholds


def generate_pseudo_labels_multihead(model, images, device, args, classwise_thresholds):
    """
    Generate pseudo-labels using multi-head ensemble voting.
    Returns confident pseudo-labels based on class-wise thresholds.
    """
    model.eval()
    with torch.no_grad():
        # Get predictions from all heads
        if args.use_multi_head:
            all_heads_output = model(images, return_all_heads=True)  # List of [B, C, H, W]
            
            # Convert to probabilities
            all_heads_probs = [F.softmax(head, dim=1) for head in all_heads_output]
            
            # Mean voting
            ensemble_probs = torch.stack(all_heads_probs).mean(dim=0)  # [B, C, H, W]
        else:
            output = model(images)
            ensemble_probs = F.softmax(output, dim=1)
        
        # Get confidence and pseudo-labels
        confidence, pseudo_labels = ensemble_probs.max(dim=1)  # [B, H, W]
        
        # Create confidence mask using class-wise thresholds
        B, H, W = pseudo_labels.shape
        confident_mask = torch.zeros(B, H, W, dtype=torch.bool, device=device)
        
        if args.use_classwise_threshold:
            for class_id, threshold in classwise_thresholds.items():
                class_mask = (pseudo_labels == class_id) & (confidence > threshold)
                confident_mask = confident_mask | class_mask
        else:
            confident_mask = confidence > args.confidence_threshold
        
        # Count confident pixels
        num_confident = confident_mask.sum().item()
        
    model.train()
    
    return pseudo_labels, confident_mask, num_confident, ensemble_probs


def compute_class_weights_from_dataset(dataset, num_classes, ignore_label=255):
    """Compute class weights from dataset for balanced training"""
    print("\nComputing class weights from dataset...")
    
    all_labels = []
    sample_size = min(1000, len(dataset))  # Sample to save time
    indices = np.random.choice(len(dataset), sample_size, replace=False)
    
    for idx in tqdm(indices, desc="Sampling labels"):
        _, label, _, _, _ = dataset[idx]
        # Label is already a numpy array from the dataset
        if hasattr(label, 'numpy'):
            label = label.numpy()
        label = label.flatten()
        label = label[label != ignore_label]
        all_labels.append(label)
    
    all_labels = np.concatenate(all_labels)
    
    # Compute weights
    class_weights = compute_class_weight(
        'balanced',
        classes=np.arange(num_classes),
        y=all_labels
    )
    
    print("  Class weights:")
    for i, weight in enumerate(class_weights):
        print(f"    Class {i}: {weight:.4f}")
    
    return torch.FloatTensor(class_weights)


def evaluate_model(model, dataloader, device, num_classes, ignore_label=255, exclude_background=True):
    """
    Evaluate model and compute detailed mIoU metrics
    
    Args:
        exclude_background: If True, exclude class 0 from mIoU calculation (recommended)
    
    Returns:
        dict: Contains 'miou', 'miou_with_bg', 'class_ious', 'detailed_metrics'
    """
    model.eval()
    label_trues = []
    label_preds = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating", leave=False):
            images, labels, _, _, _ = batch
            images = images.to(device)
            labels = labels.numpy()
            
            label_size = labels.shape[1:]
            
            # Forward pass (use ensemble if multi-head)
            preds = model(images, return_all_heads=False) if hasattr(model, 'num_heads') else model(images)
            preds = F.interpolate(preds, size=label_size, mode='bilinear', align_corners=True)
            preds = preds.detach().cpu().numpy()
            preds = np.argmax(preds, axis=1)
            
            for pred, label in zip(preds, labels):
                mask = label != ignore_label
                if mask.sum() > 0:
                    label_trues.append(label[mask])
                    label_preds.append(pred[mask])
    
    model.train()
    
    if len(label_trues) == 0:
        return {
            'miou': 0.0,
            'miou_with_bg': 0.0,
            'class_ious': [0.0] * num_classes,
            'detailed_metrics': None
        }
    
    # Get metrics excluding background (class 0)
    metrics_no_bg = scores(label_trues, label_preds, num_classes, exclude_background=True)
    # Get metrics including background (class 0)
    metrics_with_bg = scores(label_trues, label_preds, num_classes, exclude_background=False)
    
    class_ious = [metrics_no_bg['Class IoU'].get(i, 0.0) for i in range(num_classes)]
    
    return {
        'miou': metrics_no_bg['Mean IoU'],
        'miou_with_bg': metrics_with_bg['Mean IoU'],
        'class_ious': class_ious,
        'detailed_metrics': metrics_no_bg
    }


def main():
    args = get_arguments()
    
    # Set random seed
    np.random.seed(args.random_seed)
    torch.manual_seed(args.random_seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(args.random_seed)
    
    # Parse input size
    h, w = map(int, args.input_size.split(','))
    input_size = (h, w)
    
    # Setup device
    cudnn.enabled = True
    cudnn.benchmark = True
    device = get_device(cuda=True)
    
    # Initialize Wandb
    if not args.no_wandb:
        try:
            wandb.login()
        except:
            print("\n" + "="*60)
            print("Wandb Login Required")
            print("="*60)
            print("Please enter your Wandb API key.")
            api_key = input("API Key: ").strip()
            wandb.login(key=api_key)
        
        run_name = args.wandb_run_name or f"s4gan_v3_deeplabv3plus_{args.batch_size}bs"
        wandb.init(
            project=args.wandb_project,
            name=run_name,
            config=vars(args),
            tags=["v3", "deeplabv3plus", "multi-head", "no-discriminator"]
        )
        print(f"\nâœ“ Wandb initialized: {args.wandb_project}/{run_name}\n")
    
    # Create checkpoint directory
    os.makedirs(args.checkpoint_dir, exist_ok=True)
    
    # Load dataset
    print("="*60)
    print("Loading Salak Dataset...")
    print("="*60)
    
    # Construct path to labeled files
    labeled_files_path = os.path.join(os.path.dirname(args.class_mapping), args.labeled_files)
    print(f"Using labeled files: {labeled_files_path}")
    
    full_dataset = SalakDataSet(
        root=args.data_root,
        list_path=labeled_files_path,
        class_mapping_csv=args.class_mapping,
        module='s4gan',
        crop_size=input_size,
        scale=args.random_scale,
        mirror=args.random_mirror,
        mean=IMG_MEAN
    )
    
    dataset_size = len(full_dataset)
    print(f"\nâœ“ Total patches loaded: {dataset_size}")
    
    labeled_count = sum(1 for item in full_dataset.files if item['label'] is not None)
    unlabeled_count = dataset_size - labeled_count
    print(f"âœ“ Labeled patches: {labeled_count}")
    print(f"âœ“ Unlabeled patches: {unlabeled_count}")
    
    # Split labeled data
    labeled_indices = [i for i, item in enumerate(full_dataset.files) if item['label'] is not None]
    
    val_size = int(args.val_split * len(labeled_indices))
    train_size_labeled = len(labeled_indices) - val_size
    
    np.random.shuffle(labeled_indices)
    
    train_indices = labeled_indices[:train_size_labeled]
    val_indices = labeled_indices[train_size_labeled:]
    
    print(f"\nðŸ“Š Training Strategy:")
    print(f"  Labeled train samples: {train_size_labeled}")
    print(f"  Labeled validation samples: {val_size}")
    print(f"  Unlabeled samples: {unlabeled_count}")
    
    # Compute class weights if enabled
    class_weights = None
    if args.use_class_weights:
        # Create subset for weight computation
        train_dataset_subset = torch.utils.data.Subset(full_dataset, train_indices)
        class_weights = compute_class_weights_from_dataset(
            train_dataset_subset, args.num_classes, args.ignore_label
        )
        class_weights = class_weights.to(device)
    
    # Create samplers
    train_sampler = data.sampler.SubsetRandomSampler(train_indices)
    val_sampler = data.sampler.SubsetRandomSampler(val_indices)
    
    # Create dataloaders
    trainloader = data.DataLoader(
        full_dataset,
        batch_size=args.batch_size,
        sampler=train_sampler,
        num_workers=0,
        pin_memory=True
    )
    
    trainloader_unlabeled = data.DataLoader(
        full_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=0,
        pin_memory=True
    )
    
    valloader = data.DataLoader(
        full_dataset,
        batch_size=args.batch_size,
        sampler=val_sampler,
        num_workers=0,
        pin_memory=True
    )
    
    # Create model
    print("\n" + "="*60)
    print("Initializing DeepLabV3+ Model (V3)...")
    print("="*60)
    
    if args.use_multi_head:
        print(f"  Using Multi-Head architecture ({args.num_heads} heads)")
        model = MultiHeadDeepLabV3Plus(
            n_classes=args.num_classes,
            num_heads=args.num_heads,
            output_stride=args.output_stride,
            pretrained_backbone=True
        )
    else:
        print(f"  Using Single-Head DeepLabV3+")
        model = DeepLabV3Plus_ResNet50(
            n_classes=args.num_classes,
            output_stride=args.output_stride,
            pretrained_backbone=True
        )
    
    # Load checkpoint if provided
    start_iter = 0
    if args.restore_from is not None and os.path.isfile(args.restore_from):
        print(f"Loading checkpoint from: {args.restore_from}")
        checkpoint = torch.load(args.restore_from)
        
        if isinstance(checkpoint, dict) and 'model_state' in checkpoint:
            model.load_state_dict(checkpoint['model_state'])
            start_iter = checkpoint.get('iteration', 0)
            print(f"  âœ“ Resuming from iteration: {start_iter}")
        else:
            model.load_state_dict(checkpoint)
    
    model = nn.DataParallel(model)
    model = model.to(device)
    model.train()
    
    # Initialize EMA
    ema = None
    if args.use_ema:
        ema = ExponentialMovingAverage(model.module, decay=args.ema_decay)
        print(f"âœ“ EMA initialized with decay={args.ema_decay}")
    
    print(f"âœ“ Model initialized")
    total_params = sum(p.numel() for p in model.parameters()) / 1e6
    print(f"  Total parameters: {total_params:.2f}M")
    
    # Setup optimizer with different LRs for backbone and decoder
    if hasattr(model.module, 'get_backbone_params'):
        optimizer = torch.optim.SGD([
            {'params': model.module.get_backbone_params(), 'lr': args.learning_rate},
            {'params': model.module.get_decoder_params(), 'lr': args.learning_rate * 10}
        ], momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)
    else:
        optimizer = torch.optim.SGD(
            model.parameters(), 
            lr=args.learning_rate, 
            momentum=MOMENTUM, 
            weight_decay=WEIGHT_DECAY
        )
    
    # Learning rate scheduler
    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=args.warmup_iters,
        num_training_steps=args.num_steps
    )
    
    # Setup loss
    if args.use_combined_loss:
        criterion = CombinedLoss(
            ce_weight=args.ce_weight,
            dice_weight=args.dice_weight,
            focal_weight=args.focal_weight,
            class_weights=class_weights,
            ignore_label=args.ignore_label
        ).to(device)
        print(f"âœ“ Using Combined Loss (CE:{args.ce_weight} + Dice:{args.dice_weight} + Focal:{args.focal_weight})")
    else:
        criterion = CrossEntropy2d(ignore_label=args.ignore_label).to(device)
        print(f"âœ“ Using CrossEntropy Loss")
    
    # Get class-wise thresholds
    classwise_thresholds = get_classwise_thresholds(args.num_classes)
    print(f"\nâœ“ Class-wise confidence thresholds:")
    for cls_id, threshold in classwise_thresholds.items():
        print(f"    Class {cls_id}: {threshold:.2f}")
    
    # Stability monitoring
    best_val_miou = 0.0
    patience_counter = 0
    
    # Training loop
    print("\n" + "="*60)
    print("Starting Training (V3 - DeepLabV3+ Multi-Head)...")
    print("="*60)
    print(f"ðŸ”§ V3 Features Active:")
    print(f"  â€¢ Architecture: DeepLabV3+ ResNet50")
    print(f"  â€¢ Multi-Head: {args.num_heads if args.use_multi_head else 'Disabled'}")
    print(f"  â€¢ Combined Loss: {'Enabled' if args.use_combined_loss else 'Disabled'}")
    print(f"  â€¢ Class Weights: {'Enabled' if args.use_class_weights else 'Disabled'}")
    print(f"  â€¢ Multi-Scale: {'Enabled' if args.multi_scale else 'Disabled'}")
    print(f"  â€¢ EMA: {'Enabled' if args.use_ema else 'Disabled'}")
    print(f"  â€¢ Gradient Clipping: {args.gradient_clip}")
    print(f"  â€¢ Cosine LR Warmup: {args.warmup_iters} iters")
    print(f"  â€¢ NO Discriminator (removed)")
    print("="*60 + "\n")
    
    trainloader_iter = iter(trainloader)
    unlabeled_iter = iter(trainloader_unlabeled)
    
    pbar = tqdm(range(start_iter, args.num_steps), initial=start_iter, total=args.num_steps, desc="Training V3")
    
    for i_iter in pbar:
        
        optimizer.zero_grad()
        
        # =======================
        # 1. Supervised Loss on Labeled Data
        # =======================
        try:
            batch = next(trainloader_iter)
        except:
            trainloader_iter = iter(trainloader)
            batch = next(trainloader_iter)
        
        images, labels, _, _, _ = batch
        images = Variable(images).to(device)
        labels = Variable(labels.long()).to(device)
        
        # Multi-scale training
        if args.multi_scale and i_iter % 10 == 0:
            scale = np.random.randint(args.scale_min, args.scale_max + 1, 1)[0]
            if scale != input_size[0]:
                images = F.interpolate(images, size=(scale, scale), mode='bilinear', align_corners=True)
                labels_np = labels.cpu().numpy()
                labels = torch.from_numpy(
                    np.array([np.array(Image.fromarray(l.astype(np.uint8)).resize((scale, scale), Image.NEAREST)) 
                             for l in labels_np])
                ).long().to(device)
        
        # Forward pass
        if args.use_multi_head:
            preds = model(images, return_all_heads=False)  # Ensemble prediction
        else:
            preds = model(images)
        
        # Calculate loss
        if args.use_combined_loss:
            loss_sup, loss_ce, loss_dice, loss_focal = criterion(preds, labels)
        else:
            loss_sup = criterion(preds, labels, weight=class_weights)
            loss_ce = loss_sup.item()
            loss_dice = 0.0
            loss_focal = 0.0
        
        # =======================
        # 2. Self-Training on Unlabeled Data
        # =======================
        loss_st = 0.0
        num_confident = 0
        
        if i_iter >= SELF_TRAINING_START_ITER:
            try:
                batch_unlabeled = next(unlabeled_iter)
            except:
                unlabeled_iter = iter(trainloader_unlabeled)
                batch_unlabeled = next(unlabeled_iter)
            
            images_unlabeled, _, _, _, _ = batch_unlabeled
            images_unlabeled = Variable(images_unlabeled).to(device)
            
            # Generate pseudo-labels
            pseudo_labels, confident_mask, num_confident, _ = generate_pseudo_labels_multihead(
                model, images_unlabeled, device, args, classwise_thresholds
            )
            
            if num_confident > 0:
                # Forward pass on unlabeled data
                if args.use_multi_head:
                    preds_unlabeled = model(images_unlabeled, return_all_heads=False)
                else:
                    preds_unlabeled = model(images_unlabeled)
                
                # Apply confident mask
                pseudo_labels_masked = pseudo_labels.clone()
                pseudo_labels_masked[~confident_mask] = args.ignore_label
                
                # Calculate self-training loss
                if args.use_combined_loss:
                    loss_st, _, _, _ = criterion(preds_unlabeled, pseudo_labels_masked)
                    loss_st = args.st_loss_weight * loss_st
                else:
                    loss_st = criterion(preds_unlabeled, pseudo_labels_masked, weight=class_weights)
                    loss_st = args.st_loss_weight * loss_st
        
        # =======================
        # Total Loss and Backward
        # =======================
        if isinstance(loss_st, float):
            total_loss = loss_sup
        else:
            total_loss = loss_sup + loss_st
        
        total_loss.backward()
        
        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), args.gradient_clip)
        
        # Update
        optimizer.step()
        scheduler.step()
        
        # Update EMA
        if args.use_ema and ema is not None:
            ema.update()
        
        # =======================
        # Logging
        # =======================
        if i_iter % 100 == 0:
            loss_st_value = loss_st.item() if not isinstance(loss_st, float) else 0.0
            
            pbar.set_postfix({
                'CE': f'{loss_ce if isinstance(loss_ce, float) else loss_ce.item():.3f}',
                'Dice': f'{loss_dice if isinstance(loss_dice, float) else loss_dice.item():.3f}',
                'ST': f'{loss_st_value:.3f}',
                'ST_px': num_confident
            })
            
            if not args.no_wandb:
                log_dict = {
                    "Training Loss/Supervised": loss_sup.item(),
                    "Training Loss/Self-Training": loss_st_value,
                    "Training Loss/Total": total_loss.item(),
                    "Self-Training/Confident Pixels": num_confident,
                    "Learning Rate": optimizer.param_groups[0]['lr'],
                    "Iteration": i_iter
                }
                
                if args.use_combined_loss:
                    log_dict.update({
                        "Training Loss/CrossEntropy": loss_ce if isinstance(loss_ce, float) else loss_ce.item(),
                        "Training Loss/Dice": loss_dice if isinstance(loss_dice, float) else loss_dice.item(),
                        "Training Loss/Focal": loss_focal if isinstance(loss_focal, float) else loss_focal.item(),
                    })
                
                wandb.log(log_dict)
        
        # =======================
        # Validation Evaluation
        # =======================
        if i_iter % args.eval_every == 0 and i_iter > 0:
            pbar.write(f"\n{'='*60}")
            pbar.write(f"Evaluating at iteration {i_iter}...")
            pbar.write(f"{'='*60}")
            
            # Use EMA model for evaluation
            if args.use_ema and ema is not None:
                ema.apply_shadow()
            
            train_metrics = evaluate_model(model, trainloader, device, args.num_classes, args.ignore_label)
            val_metrics = evaluate_model(model, valloader, device, args.num_classes, args.ignore_label)
            
            train_miou = train_metrics['miou']
            val_miou = val_metrics['miou']
            
            if args.use_ema and ema is not None:
                ema.restore()
            
            gap = train_miou - val_miou
            
            pbar.write(f"\nðŸ“Š Step {i_iter} Evaluation:")
            pbar.write(f"  Train mIoU (classes 1-6): {train_miou:.4f}")
            pbar.write(f"  Train mIoU (all classes): {train_metrics['miou_with_bg']:.4f}")
            pbar.write(f"  Val mIoU (classes 1-6):   {val_miou:.4f}")
            pbar.write(f"  Val mIoU (all classes):   {val_metrics['miou_with_bg']:.4f}")
            pbar.write(f"  Train/Val Gap: {gap:.4f}")
            
            # Print additional metrics
            train_det = train_metrics['detailed_metrics']
            val_det = val_metrics['detailed_metrics']
            pbar.write(f"\n  ðŸ“ˆ Training Metrics:")
            pbar.write(f"    Overall Accuracy: {train_det['Overall Accuracy']:.4f}")
            pbar.write(f"    Producer Accuracy (Recall): {train_det['Producer Accuracy']:.4f}")
            pbar.write(f"    User Accuracy (Precision): {train_det['User Accuracy']:.4f}")
            pbar.write(f"    F1 Score: {train_det['F1 Score']:.4f}")
            pbar.write(f"\n  ðŸ“‰ Validation Metrics:")
            pbar.write(f"    Overall Accuracy: {val_det['Overall Accuracy']:.4f}")
            pbar.write(f"    Producer Accuracy (Recall): {val_det['Producer Accuracy']:.4f}")
            pbar.write(f"    User Accuracy (Precision): {val_det['User Accuracy']:.4f}")
            pbar.write(f"    F1 Score: {val_det['F1 Score']:.4f}")
            
            # Print class-wise IoU scores for validation
            pbar.write(f"\n  Val Class IoU scores:")
            for class_idx, iou_score in enumerate(val_metrics['class_ious']):
                pbar.write(f"    Class {class_idx}: {iou_score:.4f}")
            
            if gap > 0.10:
                pbar.write(f"âš ï¸  WARNING: Large train/val gap! Possible overfitting.")
            
            if not args.no_wandb:
                wandb.log({
                    "Metrics/Training mIoU": train_miou,
                    "Metrics/Validation mIoU": val_miou,
                    "Metrics/Train-Val Gap": gap,
                    "Metrics/Training Overall Accuracy": train_det['Overall Accuracy'],
                    "Metrics/Training Producer Accuracy": train_det['Producer Accuracy'],
                    "Metrics/Training User Accuracy": train_det['User Accuracy'],
                    "Metrics/Training F1 Score": train_det['F1 Score'],
                    "Metrics/Validation Overall Accuracy": val_det['Overall Accuracy'],
                    "Metrics/Validation Producer Accuracy": val_det['Producer Accuracy'],
                    "Metrics/Validation User Accuracy": val_det['User Accuracy'],
                    "Metrics/Validation F1 Score": val_det['F1 Score'],
                    "Iteration": i_iter
                })
            
            # Save best model
            if val_miou > best_val_miou:
                prev_best = best_val_miou
                best_val_miou = val_miou
                patience_counter = 0
                
                pbar.write(f"âœ“ New best validation mIoU: {best_val_miou:.4f} (previous: {prev_best:.4f})")
                pbar.write(f"  Saving best model to: {os.path.join(args.checkpoint_dir, 'best_model_ema.pth')}")
                
                if args.use_ema and ema is not None:
                    ema.apply_shadow()
                    torch.save(model.module.state_dict(), os.path.join(args.checkpoint_dir, 'best_model_ema.pth'))
                    ema.restore()
                else:
                    torch.save(model.module.state_dict(), os.path.join(args.checkpoint_dir, 'best_model.pth'))
                
                if not args.no_wandb:
                    wandb.run.summary["Best Validation mIoU"] = best_val_miou
                    wandb.run.summary["Best Model Iteration"] = i_iter
            else:
                pbar.write(f"  Current mIoU: {val_miou:.4f} (best remains: {best_val_miou:.4f})")
                patience_counter += 1
            
            # Early stopping check
            if patience_counter >= args.early_stop_patience:
                pbar.write(f"\nâš ï¸  Early stopping triggered! No improvement for {args.early_stop_patience} evaluations.")
                pbar.write(f"  Best mIoU: {best_val_miou:.4f}\n")
                pbar.write(f"  Stopping training at iteration {i_iter}\n")
                break  # Exit training loop
            
            pbar.write(f"{'='*60}\n")
        
        # =======================
        # Save Checkpoints
        # =======================
        if i_iter % args.save_latest_every == 0 and i_iter != 0:
            checkpoint = {
                'iteration': i_iter,
                'model_state': model.module.state_dict(),
                'optimizer_state': optimizer.state_dict(),
                'best_val_miou': best_val_miou,
                'scheduler_state': scheduler.state_dict(),
            }
            
            if args.use_ema and ema is not None:
                checkpoint['ema_shadow'] = ema.shadow
            
            torch.save(checkpoint, os.path.join(args.checkpoint_dir, 'latest_checkpoint.pth'))
        
        if i_iter % args.save_pred_every == 0 and i_iter != 0:
            pbar.write(f'ðŸ’¾ Saving checkpoint at iteration {i_iter}...')
            
            checkpoint = {
                'iteration': i_iter,
                'model_state': model.module.state_dict(),
                'optimizer_state': optimizer.state_dict(),
                'best_val_miou': best_val_miou,
                'scheduler_state': scheduler.state_dict(),
            }
            
            if args.use_ema and ema is not None:
                checkpoint['ema_shadow'] = ema.shadow
            
            torch.save(checkpoint, os.path.join(args.checkpoint_dir, f'checkpoint_{i_iter}.pth'))
            pbar.write(f'  âœ“ Checkpoint saved: checkpoint_{i_iter}.pth')
    
    pbar.close()
    
    # =======================
    # Final Evaluation
    # =======================
    print("\n" + "="*60)
    print("Final Evaluation...")
    print("="*60)
    
    if args.use_ema and ema is not None:
        ema.apply_shadow()
    
    train_metrics = evaluate_model(model, trainloader, device, args.num_classes, args.ignore_label)
    val_metrics = evaluate_model(model, valloader, device, args.num_classes, args.ignore_label)
    
    train_miou = train_metrics['miou']
    val_miou = val_metrics['miou']
    
    print(f"\nðŸŽ¯ Final Results:")
    print(f"  Train mIoU (classes 1-6): {train_miou:.4f}")
    print(f"  Train mIoU (all classes): {train_metrics['miou_with_bg']:.4f}")
    print(f"  Val mIoU (classes 1-6):   {val_miou:.4f}")
    print(f"  Val mIoU (all classes):   {val_metrics['miou_with_bg']:.4f}")
    print(f"  Best Validation mIoU: {best_val_miou:.4f}")
    
    # Print additional metrics
    train_det = train_metrics['detailed_metrics']
    val_det = val_metrics['detailed_metrics']
    print(f"\n  ðŸ“ˆ Final Training Metrics:")
    print(f"    Overall Accuracy: {train_det['Overall Accuracy']:.4f}")
    print(f"    Producer Accuracy (Recall): {train_det['Producer Accuracy']:.4f}")
    print(f"    User Accuracy (Precision): {train_det['User Accuracy']:.4f}")
    print(f"    F1 Score: {train_det['F1 Score']:.4f}")
    print(f"\n  ðŸ“‰ Final Validation Metrics:")
    print(f"    Overall Accuracy: {val_det['Overall Accuracy']:.4f}")
    print(f"    Producer Accuracy (Recall): {val_det['Producer Accuracy']:.4f}")
    print(f"    User Accuracy (Precision): {val_det['User Accuracy']:.4f}")
    print(f"    F1 Score: {val_det['F1 Score']:.4f}")
    
    print(f"\nðŸ“Š Final Class IoU Scores (Validation):")
    for class_idx, iou_score in enumerate(val_metrics['class_ious']):
        print(f"  Class {class_idx}: {iou_score:.4f}")
    
    print(f"\nðŸ’¡ Summary:")
    print(f"  mIoU excluding background (class 0): {val_miou:.4f}")
    print(f"  mIoU including background (class 0): {val_metrics['miou_with_bg']:.4f}")
    print(f"  Difference: {val_miou - val_metrics['miou_with_bg']:.4f}")
    print(f"Train/Val Gap: {(train_miou - val_miou):.4f}")
    
    if not args.no_wandb:
        wandb.log({
            "Metrics/Final Training mIoU": train_miou,
            "Metrics/Final Validation mIoU": val_miou,
        })
        wandb.run.summary["Final Training mIoU"] = train_miou
        wandb.run.summary["Final Validation mIoU"] = val_miou
    
    # Save final model
    if args.use_ema and ema is not None:
        torch.save(model.module.state_dict(), os.path.join(args.checkpoint_dir, 'final_model_ema.pth'))
        ema.restore()
    
    torch.save(model.module.state_dict(), os.path.join(args.checkpoint_dir, 'final_model.pth'))
    
    end = timeit.default_timer()
    print(f'\nTotal training time: {(end - start)/3600:.2f} hours')
    
    if not args.no_wandb:
        wandb.finish()
    
    print("\nâœ“ Training V3 completed!")
    print(f"ðŸ“Š Summary:")
    print(f"  â€¢ Best Validation mIoU: {best_val_miou:.4f}")
    print(f"  â€¢ Final Validation mIoU: {val_miou:.4f}")
    print(f"  â€¢ Expected: 0.70-0.76 (target achieved: {'YES âœ“' if best_val_miou >= 0.70 else 'NO âœ—'})")


if __name__ == '__main__':
    # Add PIL import for multi-scale
    from PIL import Image
    main()
