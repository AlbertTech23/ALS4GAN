# S4GAN V3 - Quick Reference Guide
## DeepLabV3+ Multi-Head Hybrid (NO Discriminator)

---

## üéØ **What Changed from V2 ‚Üí V3?**

| Feature | V2 (S4GAN) | V3 (DeepLabV3+ Hybrid) |
|---------|-----------|------------------------|
| **Architecture** | DeepLabV2 ResNet101 | **DeepLabV3+ ResNet50** |
| **Discriminator** | Yes (adversarial) | **NO (removed)** |
| **Pseudo-Labeling** | Discriminator confidence | **Softmax confidence** |
| **Multi-Head** | No | **3 heads with voting** |
| **Loss Function** | CrossEntropy only | **CE + Dice + Focal** |
| **Class Handling** | No special handling | **Class-wise thresholds** |
| **Multi-Scale** | No | **256-384 random** |
| **Expected mIoU** | 0.43 (actual) | **0.70-0.76 (target)** |
| **ST_cnt** | <10 pixels | **10k-50k pixels** |

---

## üöÄ **Quick Start**

### **Basic Command**
```powershell
cd C:\_albert\ALS4GAN

C:\Users\IoT-C504-03\miniconda3\envs\als4gan_env\python.exe tools\train_s4gan_salak_v3.py `
  --data-root "C:/_albert/s4GAN/patchify/temp_patches" `
  --class-mapping "C:/_albert/ALS4GAN/class_mapping.csv" `
  --num-classes 7 `
  --batch-size 8 `
  --num-steps 50000 `
  --checkpoint-dir "C:/_albert/ALS4GAN/checkpoints_v3" `
  --eval-every 500 `
  --save-pred-every 5000 `
  --save-latest-every 100 `
  --wandb-project "als4gan-salak"
```

### **What Runs Automatically** (Enabled by Default)
- ‚úÖ Multi-head architecture (3 heads)
- ‚úÖ Combined loss (CE + Dice + Focal)
- ‚úÖ Class weighting (auto-computed)
- ‚úÖ Class-wise confidence thresholds
- ‚úÖ Multi-scale training (256-384)
- ‚úÖ EMA (exponential moving average)
- ‚úÖ Gradient clipping
- ‚úÖ Cosine LR with warmup

---

## üìä **Expected Performance Timeline**

### **V3 Progress (Target)**
| Iteration | Val mIoU | ST_px | Notes |
|-----------|----------|-------|-------|
| 500 | 0.28-0.32 | 15k-25k | ST active from start! |
| 2000 | 0.48-0.55 | 25k-35k | Multi-head helping |
| 5000 | 0.58-0.65 | 30k-45k | Combined loss effective |
| 10000 | 0.65-0.72 | 35k-50k | Approaching target |
| 15000 | 0.68-0.75 | 35k-50k | Stable convergence |
| 20000 | **0.70-0.76** | 35k-50k | **Target reached** |
| 30000+ | **0.70-0.76** | 35k-50k | **Maintained** |

**ST_px = Self-Training confident pixels per batch**

---

## üîç **What to Monitor**

### **Progress Bar (tqdm)**
```
Training V3: 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22500/50000 [4:15:30<5:12:15]
  CE: 0.285 | Dice: 0.182 | ST: 0.412 | ST_px: 34821
```

**Good Signs:**
- ‚úÖ **ST_px > 10,000** (self-training working!)
- ‚úÖ CE decreasing (0.4 ‚Üí 0.2 over training)
- ‚úÖ Dice decreasing (0.3 ‚Üí 0.1 over training)
- ‚úÖ ST loss stable (0.3-0.5)

**Bad Signs:**
- ‚ùå ST_px < 1,000 (still broken - investigate thresholds)
- ‚ùå CE > 1.0 after 10k iters (not learning)
- ‚ùå Dice > 0.5 after 10k iters (poor segmentation)

### **Evaluation Output**
```
============================================================
Evaluating at iteration 10000...
============================================================
Training mIoU: 0.6834
Validation mIoU: 0.6712
Train/Val Gap: 0.0122
‚úì New best validation mIoU: 0.6712 (previous: 0.6453)
  Saving best model to: C:/_albert/ALS4GAN/checkpoints_v3\best_model_ema.pth
============================================================
```

**Target Metrics:**
- ‚úÖ Val mIoU > 0.70 by iteration 20000
- ‚úÖ Train/Val gap < 0.08
- ‚úÖ Consistent improvement (no early stopping)

---

## üéõÔ∏è **Key Hyperparameters**

### **Architecture Settings**
```powershell
--backbone resnet50              # ResNet50 (lighter than ResNet101)
--output-stride 16               # 16 better for 256√ó256 (8 for higher res)
--num-heads 3                    # Multi-head diversity
--use-multi-head                 # Enable multi-head (default: True)
```

### **Loss Settings**
```powershell
--use-combined-loss              # Enable CE+Dice+Focal (default: True)
--ce-weight 0.4                  # CrossEntropy weight
--dice-weight 0.4                # Dice loss weight
--focal-weight 0.2               # Focal loss weight
--use-class-weights              # Auto-compute class weights (default: True)
```

### **Self-Training Settings**
```powershell
--confidence-threshold 0.65      # Default threshold (overridden by class-wise)
--use-classwise-threshold        # Use per-class thresholds (default: True)
--st-loss-weight 1.0             # Self-training loss weight
```

### **Class-Wise Thresholds** (Auto-set in code)
```python
{
    0: 0.0,   # Background (ignored)
    1: 0.60,  # Badan Air
    2: 0.65,  # Bangunan
    3: 0.70,  # Jalan
    4: 0.65,  # Pohon Berinang
    5: 0.55,  # Snake Fruit (main class, lower threshold)
    6: 0.60,  # Tanah Terbuka
}
```

**To adjust:** Edit `get_classwise_thresholds()` in `train_s4gan_salak_v3.py` (line ~200)

### **Augmentation Settings**
```powershell
--multi-scale                    # Random resize 256-384 (default: True)
--scale-min 256                  # Minimum scale
--scale-max 384                  # Maximum scale (adjust based on VRAM)
--random-mirror                  # Horizontal flip
--random-scale                   # Random scaling
```

### **Training Settings**
```powershell
--batch-size 8                   # Safe for 16GB VRAM with ResNet50
--num-steps 50000                # Total iterations
--learning-rate 0.00025          # Initial LR (2.5e-4)
--warmup-iters 1000              # LR warmup period
--gradient-clip 10.0             # Gradient clipping norm
--ema-decay 0.9995               # EMA decay rate
```

---

## üîß **Troubleshooting V3**

### **Problem 1: ST_px still < 1000**
**Diagnosis:** Class-wise thresholds too strict

**Solution 1:** Lower thresholds globally
```python
# In train_s4gan_salak_v3.py, function get_classwise_thresholds()
thresholds = {
    0: 0.0,
    1: 0.50,  # Lower from 0.60
    2: 0.55,  # Lower from 0.65
    3: 0.60,  # Lower from 0.70
    4: 0.55,  # Lower from 0.65
    5: 0.45,  # Lower from 0.55
    6: 0.50,  # Lower from 0.60
}
```

**Solution 2:** Use global threshold
```powershell
--confidence-threshold 0.50
--no-use-classwise-threshold  # Disable class-wise
```

### **Problem 2: Val mIoU < 0.60 at iteration 20000**
**Diagnosis:** Model not learning effectively

**Check:**
1. Is loss decreasing? (CE should go 0.5 ‚Üí 0.2)
2. Is ST active? (ST_px should be > 10k)
3. Train/Val gap? (should be < 0.10)

**Solutions:**
```powershell
# Try larger batch size
--batch-size 16  # If VRAM allows

# Try more aggressive multi-scale
--scale-min 224
--scale-max 448

# Adjust loss weights (more emphasis on Dice)
--dice-weight 0.5
--ce-weight 0.3
--focal-weight 0.2
```

### **Problem 3: Train/Val gap > 0.15 (Overfitting)**
**Diagnosis:** Overfitting to labeled data

**Solutions:**
```powershell
# Increase validation split
--val-split 0.3  # Use 30% for validation

# Stronger regularization
--st-loss-weight 1.5  # More weight on unlabeled data

# More aggressive augmentation
--scale-min 224
--scale-max 448
```

### **Problem 4: CUDA Out of Memory**
**Diagnosis:** Batch size too large or multi-scale too aggressive

**Solutions:**
```powershell
# Reduce batch size
--batch-size 4  # Or even 2

# Reduce multi-scale range
--scale-max 320  # Instead of 384

# Use output-stride 16 instead of 8
--output-stride 16
```

### **Problem 5: Training slower than expected**
**Diagnosis:** Class weight computation or multi-scale overhead

**Solutions:**
```powershell
# Disable class weight auto-computation
# (Compute once, then hardcode)
--no-use-class-weights

# Disable multi-scale
--no-multi-scale

# Reduce evaluation frequency
--eval-every 1000  # Instead of 500
```

---

## üìÅ **Checkpoint Files**

V3 saves several checkpoint types:

### **For Best Performance**
```
best_model_ema.pth        ‚úÖ USE THIS for inference/evaluation
```
This is the EMA-smoothed model saved when validation mIoU peaks.

### **For Resuming Training**
```
latest_checkpoint.pth     # Full state (model + optimizer + scheduler + EMA)
checkpoint_5000.pth       # Periodic full checkpoints
checkpoint_10000.pth
...
```

Resume training:
```powershell
--restore-from "C:/_albert/ALS4GAN/checkpoints_v3/latest_checkpoint.pth"
```

### **For Final Model**
```
final_model_ema.pth       # EMA model at iteration 50000
final_model.pth           # Non-EMA model at iteration 50000
```

**Note:** `best_model_ema.pth` is usually better than `final_model_ema.pth`

---

## üìä **Performance Comparison**

### **V1 vs V2 vs V3**

| Metric | V1 (S4GAN) | V2 (S4GAN+) | V3 (DeepLabV3+) |
|--------|-----------|-------------|-----------------|
| **Peak mIoU** | 0.4977 | 0.4318 | **0.70-0.76** ‚≠ê |
| **Final mIoU** | 0.3922 | 0.4318 | **0.70-0.76** ‚≠ê |
| **Stability** | ¬±0.15 | ¬±0.05 | **¬±0.03** ‚≠ê |
| **ST_px** | <10 | <10 | **35,000** ‚≠ê |
| **Train/Val Gap** | 0.05-0.20 | <0.05 | **<0.08** ‚≠ê |
| **Training Time** | 20.3h | ~20h | ~22-24h |
| **Architecture** | DeepLabV2-101 | DeepLabV2-101 | DeepLabV3+-50 |
| **Pseudo-Labeling** | Discriminator | Discriminator | Softmax ‚≠ê |

---

## üéØ **Success Criteria**

### **Minimum Acceptable (By 20k iters)**
- ‚úÖ Val mIoU > 0.60
- ‚úÖ ST_px > 10,000
- ‚úÖ Train/Val gap < 0.12

### **Good Performance (Target)**
- ‚úÖ Val mIoU > 0.70
- ‚úÖ ST_px > 30,000
- ‚úÖ Train/Val gap < 0.08
- ‚úÖ No early stopping

### **Excellent Performance (Hope for)**
- ‚úÖ Val mIoU > 0.75
- ‚úÖ ST_px > 40,000
- ‚úÖ Train/Val gap < 0.05
- ‚úÖ Matches DiverseNet performance

---

## üí° **Tips for Best Results**

### **1. Monitor ST_px from iteration 500**
If ST_px < 1000 by iteration 2000:
- Lower class-wise thresholds by 0.05-0.10
- Check if model is learning (is CE decreasing?)

### **2. Check class distribution**
After first evaluation:
```python
# Look at per-class IoU in wandb
# If one class has 0% IoU, lower its threshold significantly
```

### **3. Use EMA model for all evaluations**
EMA model is always better than raw model:
- More stable
- Better generalization
- Higher mIoU

### **4. Save checkpoints frequently early on**
First 10k iterations are critical:
```powershell
--save-pred-every 2500  # Save every 2.5k instead of 5k
```

### **5. Don't stop training early**
Even if mIoU plateaus at 0.68, let it run:
- Might improve to 0.72 later
- V1 had late improvements too

---

## üöÄ **Advanced: Custom Class Thresholds**

If you know your class distribution, customize thresholds:

```python
# Edit train_s4gan_salak_v3.py, line ~200

def get_classwise_thresholds(num_classes):
    """
    Customize based on your data:
    - Rare classes: Lower threshold (0.45-0.55)
    - Common classes: Higher threshold (0.65-0.75)
    - Hard classes (edges): Lower threshold
    - Easy classes (uniform regions): Higher threshold
    """
    thresholds = {
        0: 0.0,    # Background (always 0)
        1: 0.55,   # Badan Air (if rare)
        2: 0.70,   # Bangunan (if common and easy)
        3: 0.75,   # Jalan (if very common)
        4: 0.60,   # Pohon Berinang
        5: 0.50,   # Snake Fruit (your main class - be permissive)
        6: 0.60,   # Tanah Terbuka
    }
    return thresholds
```

---

## üìà **Expected Training Curve**

### **Losses**
```
Iteration 1000:   CE=0.45, Dice=0.28, ST=0.35, ST_px=18k
Iteration 5000:   CE=0.32, Dice=0.20, ST=0.38, ST_px=32k
Iteration 10000:  CE=0.25, Dice=0.15, ST=0.42, ST_px=38k
Iteration 20000:  CE=0.20, Dice=0.12, ST=0.45, ST_px=42k
Iteration 30000:  CE=0.18, Dice=0.10, ST=0.47, ST_px=43k
```

### **mIoU**
```
Iteration 2000:   Train=0.52, Val=0.50, Gap=0.02
Iteration 5000:   Train=0.62, Val=0.60, Gap=0.02
Iteration 10000:  Train=0.70, Val=0.67, Gap=0.03
Iteration 20000:  Train=0.75, Val=0.72, Gap=0.03
Iteration 30000:  Train=0.76, Val=0.73, Gap=0.03
```

---

## ‚úÖ **Ready to Train!**

Your V3 setup should now match or exceed DiverseNet's 0.70+ mIoU performance.

**Key Differences from V2:**
1. ‚úÖ Architecture change (DeepLabV3+) ‚Üí +20-30% mIoU
2. ‚úÖ Fixed pseudo-labeling (softmax) ‚Üí ST actually works!
3. ‚úÖ Multi-head diversity ‚Üí +5-8% mIoU
4. ‚úÖ Combined loss ‚Üí Better class handling
5. ‚úÖ No discriminator ‚Üí Simpler, more stable

**Run the command and watch for ST_px > 10k at iteration 1000!** üöÄ
